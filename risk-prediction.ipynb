{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from finta import TA\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import compiled_tcn\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextDay(y, m, d):\n",
    "    gDate = datetime.datetime(y, m, d)\n",
    "    nextday = gDate + datetime.timedelta(days=1)\n",
    "    nDate = '{:%Y-%m-%d}'.format(nextday)\n",
    "    return nDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updown(v):\n",
    "    return 'Up' if v==1 else 'Down'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_date(date):\n",
    "    datearr = date.split('-')\n",
    "\n",
    "    newdate = datearr[0]\n",
    "\n",
    "    newdate += '-'\n",
    "    if len(datearr[1]) == 1:\n",
    "        newdate += '0'\n",
    "    newdate += datearr[1]\n",
    "\n",
    "    newdate += '-'\n",
    "    if len(datearr[2]) == 1:\n",
    "        newdate += '0'\n",
    "    newdate += datearr[2]\n",
    "\n",
    "    return newdate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0a22c1b5d82a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentiment_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GOOG\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mstock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GOOG\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"1d\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;31m#stock['date'] = [str(x)[:10] for x in stock.index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\yfinance\\multi.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(tickers, start, end, actions, threads, ignore_tz, group_by, auto_adjust, back_adjust, keepna, progress, period, show_errors, interval, prepost, proxy, rounding, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m                                    rounding=rounding, timeout=timeout)\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshared\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_DFS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0m_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;31m# download synchronously\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentimentanalysis import sentiment_analysis\n",
    "def gather_data(company, start_time, end_time):\n",
    "\n",
    "    # the time of tweets collected, change it when collecting more tweets to train the model\n",
    "    start_time = \"2018-01-01\"\n",
    "    end_time = \"2022-05-07\"\n",
    "\n",
    "    sentiment = sentiment_analysis(company)\n",
    "    \n",
    "    stock = yf.download(company, start=start_time, end=end_time, interval=\"1d\")\n",
    "    stock['date'] = [str(x)[:10] for x in stock.index]\n",
    "\n",
    "    print (stock)\n",
    "    \n",
    "    \n",
    "    \n",
    "#stock_data, label =gather_data(\"GOOG\",\"2018-01-01\",\"2022-05-07\")\n",
    "\n",
    "#stock_data, label =\n",
    "start_time = \"2018-01-01\"\n",
    "end_time = \"2022-05-07\"\n",
    "\n",
    "sentiment = sentiment_analysis(\"GOOG\")\n",
    "    \n",
    "stock = yf.download(\"GOOG\", start=start_time, end=end_time, interval=\"1d\")\n",
    "    #stock['date'] = [str(x)[:10] for x in stock.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sentiment.merge(stock, how='inner', on='date')\n",
    "    data.drop(data[(data.pos == 0) & (data.neg == 0)].index, inplace=True)\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    # data = stock\n",
    "    last_idx = data.shape[0] - 1\n",
    "\n",
    "    hist_data = []\n",
    "    for i in range(20, last_idx):\n",
    "        hist_data.append([data['Open'][i], data['High'][i], data['Low'][i], data['Close'][i], data['Adj Close'][i],\n",
    "                          data['Volume'][i]])\n",
    "\n",
    "    # Technical Indicators\n",
    "    stock = data\n",
    "    ema = TA.EMA(stock, 50).round(decimals=8)\n",
    "    adx = TA.ADX(stock).round(decimals=8)\n",
    "    macd = TA.MACD(stock).round(decimals=8)\n",
    "    rsi = TA.RSI(stock).round(decimals=8)\n",
    "    sar = TA.SAR(stock).round(decimals=8)\n",
    "    cci = TA.CCI(stock).round(decimals=8)\n",
    "    stoch = TA.STOCH(stock).round(decimals=8)\n",
    "    bop = TA.BOP(stock).round(decimals=8)\n",
    "    do = TA.DO(stock).round(decimals=8)\n",
    "    vwap = TA.VWAP(stock).round(decimals=8)\n",
    "\n",
    "    indicators = []\n",
    "    for i in range(20, last_idx):\n",
    "        ind = [ema[i], adx[i], macd['MACD'][i], rsi[i], sar[i], cci[i], stoch[i], bop[i], do['MIDDLE'][i], vwap[i]]\n",
    "        indicators.append(ind)\n",
    "\n",
    "    # predicted needed data\n",
    "    stock_data = np.concatenate((hist_data, indicators), 1)\n",
    "\n",
    "    # add label\n",
    "    label = []\n",
    "    for i in range(20, last_idx):\n",
    "        if data['Open'][i + 1] >= data['Open'][i]:\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    label = np.array(label)\n",
    "\n",
    "    print(\"Num of class=0: \", sum(i == 0 for i in label))\n",
    "    print(\"Num of class=1: \", sum(i == 1 for i in label))\n",
    "\n",
    "    # Min-Max normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(stock_data)\n",
    "\n",
    "    stock_data = scaler.transform(stock_data)\n",
    "\n",
    "    # add sentiment analysis\n",
    "    pos = np.array(data[20:last_idx]['pos']).reshape((data[20:last_idx]['pos'].shape[0], 1))\n",
    "    neg = np.array(data[20:last_idx]['neg']).reshape((data[20:last_idx]['neg'].shape[0], 1))\n",
    "    stock_data = np.concatenate((stock_data, pos, neg), 1)\n",
    "\n",
    "    return stock_data, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = \"2018-01-01\"\n",
    "end_time = \"2022-05-07\"\n",
    "sentiment = pd.read_csv(\"data/rates/ratesGOOG.csv\")\n",
    "sentiment.date = [process_date(x) for x in sentiment.date]\n",
    "sentiment = sentiment[['date', 'pos', 'neg']]\n",
    "print(sentiment.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knn_model(X_train, X_test, y_train, y_test):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return acc, knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logisticReg_model(X_train, X_test, y_train, y_test):\n",
    "    lr = LogisticRegression(random_state = 0)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    return acc, lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree_model(X_train, X_test, y_train, y_test):\n",
    "    dt_entropy = RandomForestClassifier(max_depth=3, random_state=0)\n",
    "    dt_entropy.fit(X_train, y_train)\n",
    "    y_pred = dt_entropy.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    return acc, dt_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def svm_model(X_train, X_test, y_train, y_test):\n",
    "    svm = sklearn.svm.SVC(kernel='rbf')\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    return acc, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def naiveBayes_model(X_train, X_test, y_train, y_test):\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred = gnb.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    return acc, gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train, X_test, y_train, y_test, epochs):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=epochs, verbose=0)\n",
    "\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = []\n",
    "    for prob in y_pred_prob:\n",
    "        if prob < 0.5:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return acc, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tcn_model(X_train, X_test, y_train, y_test):\n",
    "    tcn = compiled_tcn(\n",
    "        return_sequences=False,\n",
    "        num_feat=1,\n",
    "        num_classes=2,\n",
    "        nb_filters=30,\n",
    "        kernel_size=2,\n",
    "        dilations=[2 ** i for i in range(9)],\n",
    "        nb_stacks=1,\n",
    "        max_len=X_train.shape[1],\n",
    "        use_skip_connections=True,\n",
    "        use_weight_norm=True,\n",
    "        dropout_rate=0.1)\n",
    "\n",
    "    tcn.fit(X_train, y_train, epochs=30, verbose=0)\n",
    "\n",
    "    y_pred_prob = tcn.predict(X_test)\n",
    "    y_pred = []\n",
    "    for prob in y_pred_prob:\n",
    "        if prob[0] > prob[1]:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return acc, tcn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def risk_prediction_ml(X_train, X_test, y_train, y_test, today):\n",
    "    acc_list = {}\n",
    "    pred = {}\n",
    "    acc_list['knn'], knn = knn_model(X_train, X_test, y_train, y_test)\n",
    "    acc_list['svm'], svm = svm_model(X_train, X_test, y_train, y_test)\n",
    "    acc_list['dt'], dt = decisionTree_model(X_train, X_test, y_train, y_test)\n",
    "    acc_list['lr'], lr = logisticReg_model(X_train, X_test, y_train, y_test)\n",
    "    acc_list['nb'], nb = naiveBayes_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    today = np.array(today).reshape(1, today.shape[0])\n",
    "    pred['knn'] = knn.predict(today)[0]\n",
    "    pred['svm'] = svm.predict(today)[0]\n",
    "    pred['dt'] = dt.predict(today)[0]\n",
    "    pred['lr'] = lr.predict(today)[0]\n",
    "    pred['nb'] = nb.predict(today)[0]\n",
    "\n",
    "    return acc_list, pred\n",
    "\n",
    "\n",
    "def risk_prediction_lstm(X_train, X_test, y_train, y_test, today):\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = y_train.reshape((y_train.shape[0], 1))\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = y_test.reshape((y_test.shape[0], 1))\n",
    "\n",
    "    #     print(\"shape: \", X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    acc_lstm, lstm = LSTM_model(X_train, X_test, y_train, y_test, epochs=50)\n",
    "\n",
    "    today = np.array(today).reshape(1, today.shape[0], 1)\n",
    "    pred = 0 if lstm.predict(today)[0] < 0.5 else 1\n",
    "\n",
    "    return acc_lstm, pred\n",
    "\n",
    "\n",
    "def risk_prediction_tcn(X_train, X_test, y_train, y_test, today):\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    y_train = y_train.reshape((y_train.shape[0], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    y_test = y_test.reshape((y_test.shape[0], 1))\n",
    "\n",
    "    acc_tcn, tcn = tcn_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    today = np.array(today).reshape(1, today.shape[0], 1)\n",
    "    prob = tcn.predict(today)[0]\n",
    "    pred = 0 if prob[0] > prob[1] else 1\n",
    "\n",
    "    return acc_tcn, pred\n",
    "\n",
    "\n",
    "\n",
    "def get_today_info(company):\n",
    "    today = date.today().strftime(\"%Y-%m-%d\")\n",
    "    y, m, d = int(today.split('-')[0]), int(today.split('-')[1]), int(today.split('-')[2])\n",
    "\n",
    "    words, pos, neg = date_prediction(company, y, m, d)\n",
    "\n",
    "    nextday = getNextDay(y, m, d)\n",
    "    data = yf.download(company, start=start_time, end=nextday, interval=\"1d\")\n",
    "    data['Date'] = data.index\n",
    "\n",
    "    last_idx = data.shape[0] - 1\n",
    "\n",
    "    stock = data\n",
    "    ema = TA.EMA(stock, 50).round(decimals=8)\n",
    "    adx = TA.ADX(stock).round(decimals=8)\n",
    "    macd = TA.MACD(stock).round(decimals=8)\n",
    "    rsi = TA.RSI(stock).round(decimals=8)\n",
    "    sar = TA.SAR(stock).round(decimals=8)\n",
    "    cci = TA.CCI(stock).round(decimals=8)\n",
    "    stoch = TA.STOCH(stock).round(decimals=8)\n",
    "    bop = TA.BOP(stock).round(decimals=8)\n",
    "    do = TA.DO(stock).round(decimals=8)\n",
    "    vwap = TA.VWAP(stock).round(decimals=8)\n",
    "\n",
    "    hist_data = []\n",
    "    for i in range(20, last_idx):\n",
    "        hist_data.append([data['Open'][i], data['High'][i], data['Low'][i], data['Close'][i], data['Adj Close'][i],\n",
    "                          data['Volume'][i]])\n",
    "    indicators = []\n",
    "    for i in range(20, last_idx + 1):\n",
    "        ind = [ema[i], adx[i], macd['MACD'][i], rsi[i], sar[i], cci[i], stoch[i], bop[i], do['MIDDLE'][i], vwap[i]]\n",
    "        indicators.append(ind)\n",
    "\n",
    "    stock_data = np.concatenate((hist_data, indicators[:-1]), 1)\n",
    "    today_data = [data['Open'][last_idx], data['High'][last_idx], data['Low'][last_idx], data['Close'][last_idx],\n",
    "                  data['Adj Close'][last_idx], data['Volume'][last_idx]] + indicators[-1]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(stock_data)\n",
    "\n",
    "    stock_data = scaler.transform(stock_data)\n",
    "    today_data = scaler.transform([today_data])[0]\n",
    "\n",
    "    today_data = list(today_data) + [pos, neg]\n",
    "\n",
    "    # generate wordcloud\n",
    "    keywords = company_list + ['tsla', 'stock', 'tickers']\n",
    "    for c in keywords:\n",
    "        words = words.replace(c.lower(), \"\")\n",
    "\n",
    "        # make wordcloud\n",
    "    if len(words) != 0:\n",
    "        wc = WordCloud(\n",
    "            max_words=200,\n",
    "            background_color='white',\n",
    "            width=2000,\n",
    "            height=1200\n",
    "        )\n",
    "\n",
    "        word_cloud = wc.generate(words)\n",
    "        word_cloud.background_color = 'white'\n",
    "        plt.imshow(word_cloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig('assets/wordcloud-{}.png'.format(company), dpi=300)\n",
    "        # plt.show()\n",
    "\n",
    "    return today_data\n",
    "\n",
    "\n",
    "def risk_prediction_all(company, start_time, end_time):\n",
    "    stock_data, label = gather_data(company, start_time, end_time)\n",
    "    today = get_today_info(company)\n",
    "    today = np.array(today)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        stock_data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "    acc_list, pred_list = risk_prediction_ml(X_train, X_test, y_train, y_test, today)\n",
    "    acc_list['lstm'], pred_list['lstm'] = risk_prediction_lstm(X_train, X_test, y_train, y_test, today)\n",
    "    acc_list['tcn'], pred_list['tcn'] = risk_prediction_tcn(X_train, X_test, y_train, y_test, today)\n",
    "\n",
    "    model = ['TCN', 'LSTM', 'Random Forest', 'Logistic Regression', 'SVM', 'Naive Bayes', 'KNN']\n",
    "    acc = [acc_list['tcn'], acc_list['lstm'], acc_list['dt'], acc_list['lr'], acc_list['svm'], acc_list['nb'],\n",
    "           acc_list['knn']]\n",
    "    pred = [updown(pred_list['tcn']), updown(pred_list['lstm']), updown(pred_list['dt']), updown(pred_list['lr']),\n",
    "            updown(pred_list['svm']), updown(pred_list['nb']), updown(pred_list['knn'])]\n",
    "    df = pd.DataFrame(data={'Model': model, 'Accuracy': acc, 'Up/Down prediction': pred})\n",
    "    print(df)\n",
    "\n",
    "    df.to_csv('data/updown-prediction-{}.csv'.format(company), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
